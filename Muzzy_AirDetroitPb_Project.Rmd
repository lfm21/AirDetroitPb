---
title: How has the "Motor City" of Detroit, MI, area experienced a decrease in air
  particulate lead over time?
author: "Laurie Muzzy"
fontsize: 12pt
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
  word_document: default
mainfont: Times New Roman
geometry: margin=2.54cm
subtitle: https://github.com/lfm21/AirDetroitPb
abstract: Tetraethyl lead (TEL) was completely phased out of gasoline in 1995; air
  lead levels have decreased 98% from 1980 to 2014. In addition to dramatically decreased airborne lead concentrations, another indicator of progress in the reduction of
  airborne lead in the environment is the drop in children's blood lead levels over
  time. Since the late 1970s, average blood lead concentration for children aged 1
  to 5 have dropped significantly, from about 15 micrograms per deciliter (µg/dL)
  to less than 1 µg/dL. On September 16, 2016, EPA announced its decision to retain,
  without revision, the national ambient air quality standards (NAAQS) for lead of
  0.15 ug/m3, in terms of a 3-month average concentration. I would like to see how
  the Detroit, MI, area (known as Motor City) has experienced a decrease in air lead
  levels over time.
---

<Information in these brackets are used for annotating the RMarkdown file. They will not appear in the final version of the PDF document>

\newpage
\tableofcontents 
\newpage
\listoftables 
\newpage
\listoffigures 
\newpage

<Setup the global options for the R chunks in your document>

<Note: set up autoreferencing for figures and tables in your document>

```{r setup, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Set your working directory
setwd("~/Desktop/Envtl_Data_Analytics/AirDetroitPb")

# Load your packages
knitr::opts_chunk$set(echo = TRUE)
library(readr)
devtools::install_github("Nowosad/spDataLarge")
library(dplyr)
library(tidyverse)
library(tidyr)
library(tidyselect)
library(lubridate)
library(ggplot2)
library(viridis)
library(viridisLite)
library(gridExtra)
library(colorRamps)
library(lsmeans)
library(nlme)
library(multcompView)
library(trend)
library(corrplot)
library(xtable)
#install.packages("sf")
#install.packages("raster")
#install.packages("spData")
library(leaflet)
library(mapview)
library(sf)
library(spData)
library(raster)
library(knitr)
getwd()

# Set your ggplot theme
Pbtheme <- theme_minimal(base_size = 11) +
theme(axis.text = element_text(color = "dark gray"),
      legend.position = "bottom")
  
theme_set(Pbtheme)

```

# Research Question and Rationale
<Paragraph detailing the rationale for your analysis. What is the significant application and/or interest in this topic? Connect to environmental topic(s)/challenge(s).>

Children under 6 are extremely vulnerable to lead poisoning, the effects of which show up later in life, and cannot be remediated. Lead exposure and poisoning therefore has large public health, social, and economic ramifications. Tetraethyl lead (TEL) was an additive in gasoline for decades, but growing public health pressures in the United States caused it to be completely phased out of gasoline by 1995. With this reduction of lead in nearly all gasoline, air lead levels have decreased 98% from 1980 to 2014.  In 2016, the U.S. EPA announced revised national ambient air quality standards (NAAQS) for lead of 0.15 ug/m3, in terms of a 3-month average concentration. Despite this ridiculously low concentration standard, and the dramatic decrease in the past four decades, lead still persists in the environment. Lead in paint has been banned in the U.S., but certain airplanes are still permitted to use leaded fuel. My research is interested in what lead concentrations are still extant in the air, especially in a larger American cities, where automobiles are the most common mode of transportation.

<Paragraph detailing your research question(s) and goals. What do you want to find out? Include a sentence (or a few) on the dataset you are using to answer this question - just enough to give your reader an idea of where you are going with the analysis.>

How has the "Motor City" Detroit, MI, area experienced a decrease in air lead levels over time?**** Where in the city has lead in air been sampled?**** Are the sample sites in the city at sites where vulnerable populations could be exposed?

\newpage

# Dataset Information
<Information on how the dataset for this analysis were collected, the data contained in the dataset, and any important pieces of information that are relevant to your analyses. This section should contain much of same information as the README file for the dataset but formatted in a way that is more narrative.>

The files were downloaded from https://www.epa.gov/outdoor-air-quality-data/download-daily-data and accessed on 2019-04-04. The area is Detroit-Warren-Dearborn in Michigan, USA. Lead(Pb) in air has been monitored every 12 days, since 1981 in Detroit-Dearborn-Warren area; there is a gap in data collection in the early 2000s. Files in .csv format were downloaded for every year available, which was 1981 until 2018. All files were accessed as .csv files, and named EPAair_Pb_Detroit_YEAR.csv
Shapefiles of neighborhoods in the area were downloaded from michigan.gov.


Reading in all 37 datasets.
```{r read in all raw data files, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
Pb1981 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1981_raw.csv")
Pb1982 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1982_raw.csv")
Pb1983 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1983_raw.csv")
Pb1984 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1984_raw.csv")
Pb1985 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1985_raw.csv")
Pb1986 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1986_raw.csv")
Pb1987 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1987_raw.csv")
Pb1988 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1988_raw.csv")
Pb1989 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1989_raw.csv")
Pb1990 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1990_raw.csv")
Pb1991 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1991_raw.csv")
Pb1992 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1992_raw.csv")
Pb1993 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1993_raw.csv")
Pb1994 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1994_raw.csv")
Pb1995 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1995_raw.csv")
Pb1996 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1996_raw.csv")
Pb1997 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1997_raw.csv")
Pb1998 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1998_raw.csv")
Pb1999 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit1999_raw.csv")
Pb2000 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2000_raw.csv")
Pb2001 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2001_raw.csv")
Pb2002 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2002_raw.csv")
Pb2003 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2003_raw.csv")
Pb2004 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2004_raw.csv")
Pb2005 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2005_raw.csv")
Pb2006 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2006_raw.csv")
Pb2007 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2007_raw.csv")
Pb2008 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2008_raw.csv")
Pb2009 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2009_raw.csv")
Pb2010 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2010_raw.csv")
Pb2011 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2011_raw.csv")
Pb2012 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2012_raw.csv")
Pb2013 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2013_raw.csv")
Pb2014 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2014_raw.csv")
Pb2015 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2015_raw.csv")
Pb2016 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2016_raw.csv")
Pb2017 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2017_raw.csv")
Pb2018 <- read.csv("./EPA_Pb_data_raw/EPA_Pb_Detroit2018_raw.csv")

```

Combine all datasets using rbind.
```{r one large dataset of all years, echo=FALSE}
Pb1981to2018.gathered <- rbind(Pb1981, Pb1982, Pb1983, Pb1984, Pb1985,
                      Pb1986, Pb1987, Pb1988, Pb1989, Pb1990,
                      Pb1991, Pb1992, Pb1993, Pb1994, Pb1995,
                      Pb1996, Pb1997, Pb1998, Pb1999, Pb2000,
                      Pb2005, Pb2006, Pb2007, Pb2008, Pb2009,
                      Pb2010, Pb2011, Pb2012, Pb2013, Pb2014,
                      Pb2015, Pb2016, Pb2017, Pb2018) 
#write.csv(Pb1981to2018.gathered, row.names = FALSE, "./EPA_Pb_data_Processed/Pb1981to2018.gathered.csv")
```


Format date to Date class for gathered data frame. Convert coordinates into geometry (using crs = 4326 from http://gis-michigan.opendata.arcgis.com/datasets/D3::master-plan-neighborhoods-detroit). 
Select only useful columns to tidy up dataset.
```{r class, echo=TRUE}
library(dplyr) 
#if raster pkg loaded after dplyr, R will default to raster's select function

Pb1981to2018.gathered$Date <- as.Date(Pb1981to2018.gathered$Date, format = "%m/%d/%Y")
class(Pb1981to2018.gathered$Daily.Mean.Pb.Concentration)

Pb1981to2018.points <- st_as_sf(Pb1981to2018.gathered,
                                coords = c("SITE_LATITUDE", "SITE_LONGITUDE"),
                                crs = 4326)

#write.csv(Pb1981to2018.points, row.names = FALSE, "./EPA_Pb_data_Processed/Pb1981to2018.points.csv")

Pb1981to2018.tidy <- Pb1981to2018.points %>% 
  select(Date, Site.ID, Daily.Mean.Pb.Concentration,
         UNITS, Site.Name, geometry) %>% 
  mutate(year = year(Date), month = month(Date))

#write.csv(Pb1981to2018.tidy, row.names = FALSE, "./EPA_Pb_data_Processed/Pb1981to2018.tidy.csv")
  
```

<Add a table that summarizes your data structure. This table can be made in markdown text or inserted as a `kable` function in an R chunk. If the latter, do not include the code used to generate your table.>

##Table of processed dataset of every sample from Detroit-Dearborn-Warren area from each year (1981-2018).
```{r kable table, echo=FALSE}

PbDetroit <- head(Pb1981to2018.tidy)
AirPbDetroit.table <- kable(PbDetroit, "markdown",align = "l", 
                            caption = "EPA Air Lead in Detroit, 1981-2018", 
                            escape = TRUE) 
AirPbDetroit.table
```

\newpage
# Exploratory Data Analysis and Wrangling

<Include R chunks for 5+ lines of summary code (display code and output), 3+ exploratory graphs (display graphs only), and any wrangling you do to your dataset(s).> 

<Include text sections to accompany these R chunks to explain the reasoning behind your workflow, and the rationale for your approach.>

View the summaries of the datasets. The first dataset was from 1981, so I chose that one to examine the column names, dimensions, structure, and class type (sample date must be converted of class Date).
```{r 1 - summary of example 1981 dataframe, then of Pb1981to2018.tidy}
str(Pb1981)
colnames(Pb1981) 
class(Pb1981$Date)
head(Pb1981)
summary(Pb1981)
dim(Pb1981)

dim(Pb1981to2018.tidy)
colnames(Pb1981to2018.tidy)
```

```{r 2.1 - exploratory histogram to see Pb concentration, using 1981 dataset}

ggplot(Pb1981) +
  geom_histogram(aes(x = Daily.Mean.Pb.Concentration), bins = 45) +
  labs(title = "Air Lead in Detroit, 1981")
```

```{r 2.2 - histogram to see Pb concentration in a more recent year}

ggplot(Pb2009) +
  geom_histogram(aes(x = Daily.Mean.Pb.Concentration), bins = 100) +
  labs(title = "Air Lead in Detroit, 2009")
```

(This plot uses Site.ID instead of Site.Name because not every site is named.)
```{r 2.3 - point graph to show differences between sites and months}
#early 1980s plot
Pb1982$Date <- as.Date(Pb1982$Date, format = "%m/%d/%Y")

Pb1982.month <- select(Pb1982, Date, Site.ID, Daily.Mean.Pb.Concentration, UNITS,
         Site.Name, SITE_LATITUDE, SITE_LONGITUDE) %>% 
  mutate(year = year(Date), month = month(Date))

Pb1982.plot <- ggplot(Pb1982.month, aes(x = month,y = Daily.Mean.Pb.Concentration,
                                  color = Site.ID)) +
  geom_point(alpha = 0.75) +
  scale_fill_continuous(name = "Site code") +
  #scale_y_log10() +
  theme(legend.position = "right") +
  labs(title = "Air Lead in Detroit, 1982")
#print(Pb1982.plot)

#and a more recent year
Pb2018$Date <- as.Date(Pb2018$Date, format = "%m/%d/%Y")

Pb2018.month <- select(Pb2018, Date, Site.ID, Daily.Mean.Pb.Concentration, UNITS,
         Site.Name, SITE_LATITUDE, SITE_LONGITUDE) %>% 
  mutate(year = year(Date), month = month(Date))

Pb2018.plot <- ggplot(Pb2018.month, aes(x = month,y = Daily.Mean.Pb.Concentration,
                                  color = Site.ID)) +
  geom_point(alpha = 0.75) +
  scale_fill_continuous(name = "Site code") +
  #scale_y_log10() +
  theme(legend.position = "right") +
  labs(title = "Air Lead in Detroit, 2018")
#print(Pb2018.plot)

library(gridExtra)
grid.arrange(Pb1982.plot, Pb2018.plot, newpage = TRUE)

```

```{r look at the .shp }
Detroit.neighborhoods <- 
  st_read("./EPA_Pb_data_Raw/Detroit_nbhoods/Master_Plan_Neighborhoods_Detroit.shp")
st_crs(Pb1981to2018.points)
st_crs(Detroit.neighborhoods)
plot(Detroit.neighborhoods["NHOOD"])
mapview(Detroit.neighborhoods["NHOOD"])
mapview(Pb1981to2018.points["Site.ID"])
```


###Most of the wrangling was done earlier in document in order to show a tidy usable dataset for the kable table. But there are examples of the wrangling in the datasets below.
I chose to test the wrangling on the 1982 dataset. Then I converted the coordinate columns to geometry points so they can be plotted on a map; and added a month and year column in order to see if there's seasonal change.

```{r 3.1 create 1980s and 2010s dataframes}
#1980s
Pb1980s.gath <- rbind(Pb1981, Pb1982, Pb1983, Pb1984, Pb1985, 
                 Pb1986, Pb1987, Pb1988, Pb1989, Pb1990)

Pb1980s.gath$Date <- as.Date(Pb1980s.gath$Date, format = "%m/%d/%Y") 

Pb1980s <- Pb1980s.gath %>% select(Date, Site.ID, Daily.Mean.Pb.Concentration,
         UNITS, Site.Name, SITE_LATITUDE, SITE_LONGITUDE) %>% 
  mutate(year = year(Date), month = month(Date)) %>% 
  st_as_sf(coords = c("SITE_LATITUDE", "SITE_LONGITUDE"),crs = 4326)

#write.csv(Pb1980s, row.names = FALSE, "./EPA_Pb_data_Processed/Pb1980s.csv")
  
#2010s
Pb2010s.gath <- rbind(Pb1981, Pb1982, Pb1983, Pb1984, Pb1985, 
                 Pb1986, Pb1987, Pb1988, Pb1989, Pb1990)

Pb2010s.gath$Date <- as.Date(Pb2010s.gath$Date, format = "%m/%d/%Y") 

Pb2010s <- Pb2010s.gath %>% select(Date, Site.ID, Daily.Mean.Pb.Concentration,
         UNITS, Site.Name, SITE_LATITUDE, SITE_LONGITUDE) %>% 
  mutate(year = year(Date), month = month(Date)) %>% 
  st_as_sf(coords = c("SITE_LATITUDE", "SITE_LONGITUDE"),crs = 4326)

#write.csv(Pb2010s, row.names = FALSE, "./EPA_Pb_data_Processed/Pb2010s.csv")
```


```{r 1982 and 2018 - I PROB DONT NEED THIS wrangling example datasets}
#wrangle 1982 data frame
as.Date(Pb1982$Date, format = "%m/%d/%Y") #takes up too much space

Pb1982.month.yr <-select(Pb1982, Date, Site.ID, Site.Name,
                         Daily.Mean.Pb.Concentration, UNITS,
                         SITE_LATITUDE, SITE_LONGITUDE) %>% 
  mutate(year = year(Date), month = month(Date)) 

#wrangle 2018 data frame
as.Date(Pb2018$Date, format = "%m/%d/%Y") #takes up too much space
Pb2018month.yr <- select(Pb2018, Date, Site.ID, Site.Name,
                                    Daily.Mean.Pb.Concentration, UNITS,
                                    SITE_LATITUDE, SITE_LONGITUDE) %>% 
  mutate(year = year(Date), month = month(Date)) 
```

There are a few variables that have a Site.Id but no Site.Name. I'd like to fill the row with something.
```{r 3.2 fill in the blank/missing Site Names with something}
Pb1981to2018.tidy$Site.Name[Pb1981to2018.tidy$Site.Name == ""] <- "unlabeled site" 
```

```{r wrangle the .shp to make it usable}

 #do I need to subset out SiteID or month or something to show points clearly?
Detroit.1980smap <- ggplot() +
  geom_sf(data = Detroit.neighborhoods, col = "red") +
  geom_sf(data = Pb1980s, col = "blue") 
plot(Pb1980s$geometry)
plot(Detroit.1980smap)

mapView(Pb1980s$geometry)

Detroit.1980sjoin <- Detroit.neighborhoods %>% 
  left_join(y = Pb1980s, by = c())

#correct order to see mult datasets
ggplot() +
  geom_sf(data = state_sf_utm, color='red',size=2) +
  geom_sf(data = counties_sf_utm, aes(fill = ALAND), color = 'white')  +
  geom_sf(data = EPAair_PM_avg_sf_UTM, color='blue', size=2) + 
  scale_fill_gradient(low="yellow", high="darkgreen")

#why a left join
NWISjoin <- NWISgage_ht %>%
left_join(y = NEgagepoints, by = c('site_no', 'station_nm')) %>%
na.omit()
```

\newpage


## Analysis
<Include R chunks for 3+ statistical tests (display code and output) and 3+ final visualization graphs (display graphs only).>
###Is there a normal distribution of Daily Mean Lead Concentration values in different decades? At different sites? In different months?

A  Shapiro-Wilk test was done on a few separate datasets. The Shapiro test can only run between 3 and 5000, so the Pb1981to2018.gathered data frame is slightly too large. Instead I'll run Shapiro on one dataset from each decade that has a relatively high number of observations. T-tests were then done to determine if the mean was indeed lower than 1.0ug/m3.
For all of these years, the p-values were very low, so we can reject the null hypothesis, as the alternative is true: the mean air lead concentration in these years was less than 1.0ug/m3.
From 1984 to 1990, the mean Pb concentration was an order of magnitude lower;
```{r 4.1 -  Shapiro-Wilk test: normal distribution, echo=TRUE}
#look at dataset from the 1980s, the 90s, the 2000s, and then a more recent dataset
shapiro.test(Pb1984$Daily.Mean.Pb.Concentration)
#W = 0.88254, p-value = 3.677e-10
t.test.1smpl.1984 <- t.test(Pb1984$Daily.Mean.Pb.Concentration,
                            mu = 50, alternative = "less")
t.test.1smpl.1984 # -Inf 0.2244882, mean of x 0.206

shapiro.test(Pb1990$Daily.Mean.Pb.Concentration)
#W = 0.28426, p-value < 2.2e-16
t.test.1smpl.1990 <- t.test(Pb1990$Daily.Mean.Pb.Concentration,
                            mu = 50, alternative = "less")
t.test.1smpl.1990

shapiro.test(Pb2006$Daily.Mean.Pb.Concentration)
#W = 0.10127, p-value < 2.2e-16
t.test.1smpl.2006 <- t.test(Pb2006$Daily.Mean.Pb.Concentration,
                            mu =50, alternative = "less")
t.test.1smpl.2006
shapiro.test(Pb2017$Daily.Mean.Pb.Concentration)
#W = 0.36213, p-value < 2.2e-16
t.test.1smpl.2017 <- t.test(Pb2017$Daily.Mean.Pb.Concentration,
                            mu = 50, alternative = "less")
t.test.1smpl.2017
```

Two sites have the largest datasets because they were sampled over the longest period of time. A comparison of these datasets was done to determine how different the variance between them may be. 
First Shapiro-Wilk tests then an F test was performed. The result is that the variance between the two populations is different. From the graph, it seems like we don't have a normal distribution with these datasets, so we should proceed with tests that do not necessarily require normality.
```{r 2-way t-test on big gathered dataset, echo=TRUE}
#find which sites have most observations and filter for those; create plot to see graphical representation of count to determine distribution
summary(Pb1981to2018.points$Site.Name)

Pb.large.sets <- Pb1981to2018.points %>%
  filter(Site.Name %in% c("Southwestern H.S.",
                          "PROPERTY OWNED BY DEARBORN PUBLIC SCHOOLS"))
Pb.large.sets.plot <- 
  ggplot(Pb.large.sets, 
         aes(x = Daily.Mean.Pb.Concentration, color = Site.Name)) +
  geom_freqpoly(stat = "count", alpha = 0.5) +
  labs(x = "Lead concentration, ug/m3", y = "number of observations", title = "Sites in Detroit w/ Largest Sample Size of Air Lead Data")
print(Pb.large.sets.plot)

#statistical comparison of the 2 largest sample sets
shapiro.test(Pb.large.sets$Daily.Mean.Pb.Concentration
             [Pb.large.sets$Site.Name == "Southwestern H.S."])

shapiro.test(Pb.large.sets$Daily.Mean.Pb.Concentration
             [Pb.large.sets$Site.Name == 
                 "PROPERTY OWNED BY DEARBORN PUBLIC SCHOOLS"])

var.test(Pb.large.sets$Daily.Mean.Pb.Concentration ~
           Pb.large.sets$Site.Name)

lm.Pb.large.sets <- lm(Pb.large.sets$Daily.Mean.Pb.Concentration ~
                          Pb.large.sets$Site.Name)
summary(lm.Pb.large.sets)

```

Kruskal-Wallis tests was performed on the whole large gathered data frame (Pb1981to2018.points); the p-val was low, showing some significance of the hypothesis that Pb concentration is realted to site.
Then Kruskal was done on the data frame with the two largest sample sizes (Pb.large.sets). We see that there is a significant difference between the two groups in the largest datasets.
```{r kruskal-wallis test - non parametric, then wilcoxon for post-hoc}
#for the whole gathered dataset
Pb1981to2018.kw <- kruskal.test(Pb1981to2018.points$Daily.Mean.Pb.Concentration ~
                Pb1981to2018.points$Site.ID)
Pb1981to2018.kw
Pb1981to2018.wilcox <- 
  wilcox.test(Pb1981to2018.points$Daily.Mean.Pb.Concentration,
              mu = 50, alternative = "less")
Pb1981to2018.wilcox

#for the two large datasets
Pb.large.sets.kw <- kruskal.test(Pb.large.sets$Daily.Mean.Pb.Concentration ~
                Pb.large.sets$Site.ID)
Pb.large.sets.kw
Pb.large.sets.wilcox <- 
  wilcox.test(Pb.large.sets$Daily.Mean.Pb.Concentration,
              mu = 50, alternative = "less")
Pb.large.sets.wilcox

```

ANCOVA and was run for the whole gathered dataset of Pb concentrations from the all the sites from 1981 to 2018 to determine the level of co-variance. The linear models explained about 31% of the covariance here.
```{r ANCOVA, echo=TRUE}

#is there co-variance btwn date and site?
Pb1981to2018.ancova <- lm(data = Pb1981to2018.points, 
                          Daily.Mean.Pb.Concentration ~ Date + Site.ID)
summary(Pb1981to2018.ancova) 

#is there an interaction btwn date and site?
Pb1981to2018.interaction <- lm(data = Pb1981to2018.points, 
                               Daily.Mean.Pb.Concentration ~ Date * Site.ID)
summary(Pb1981to2018.interaction) #explains about 31% of the variance

```

graph of potential seasonal change
```{r 5.1 first final graph, echo=FALSE}

Pb1981to2018.tidy$month <- as.character(Pb1981to2018.tidy$month)
Pb1981to2018.tidy$month <- c("J", "F", "M", "A", "M", "J",
                                  "J", "A", "S", "O", "N", "D") #do I need to name vars?

xlab(expression("total Phosphorus(micrograms/L)")) +
ylab(expression("Phosphate(micrograms/L)")) +
class(Pb1981to2018.tidy$month)

Pb.seasonal <- ggplot(Pb1981to2018.tidy, 
                      aes(x = month, y = Daily.Mean.Pb.Concentration,
                              color = Daily.Mean.Pb.Concentration)) +
  geom_point(size = 1, alpha = 0.5) +
  #scale_fill_discrete(name = "0 to 1.5 mg/m3", labels = c("A", "B", "C", "D", "E")) +
  labs(title = "Seasonality of Lead in Air, Detroit, 1981-2018",
       x = "month", y = "Mean Pb concentration") +
  geom_smooth(color = "magenta", size = 0.5)
print(Pb.seasonal)#Error: Aesthetics must be either length 1 or the same as the data (5418): x


n + scale_fill_discrete(name = "Title",
labels = c("A", "B", "C", "D", "E"))

#TNplot <- ggplot(PeterPaul.chem.nutrients, aes(x = depth, y = tn_ug, color = lakename)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + #if no color specified, it gave us 2 lines, if color = "something" it gives us 1 line
  xlim(0, 10)
#print(TNplot)
```

Graph of air lead levels at places where children could be most at risk (schools, parks, church).
```{r 5.2 FIX COLORS AND COLUMN NAMES child risk graph , echo=FALSE}

#t + facet_grid(rows = vars(year), cols = vars(fl)) ??



Pb.childrisk <-  Pb1981to2018.tidy %>%
  filter(Site.Name %in% c("PROPERTY OWNED BY DEARBORN PUBLIC SCHOOLS", 
                          "Allen Park", "Eliza Downwind", "Southwestern H.S.", 
                          "NMH48217 NEW MT HERMAN CHURCH", "River Rouge"))

Pb.childrisk.plot <- ggplot(Pb.childrisk, 
                            aes(x = year, y = Daily.Mean.Pb.Concentration)) +
  geom_point(aes(), size = 1, alpha = 0.5) +
  facet_wrap(vars(Site.Name), 
             scales = "free_y", ncol = 1, strip.position = "right") +
  scale_color_viridis(option = "cividis") +
  labs(title = "Air Lead Risk to Children, Detroit parks, 1981-2018",
       x = "Year", y = "Lead Concentration ug/m3") +
  scale_y_log10() +
  theme(axis.line.x = element_line(color = "black"),
        legend.position = "bottom") 

print(Pb.childrisk.plot)

```

Can I possibly make a map??
```{r 5.3 third final graph, echo=FALSE}



```


<Include text sections to accompany these R chunks to explain the reasoning behind your workflow, rationale for your approach, and the justification of meeting or failing to meet assumptions of tests.>


\newpage

# Summary and Conclusions
<Summarize your major findings from your analyses. What conclusions do you draw from your findings? Make sure to apply this to a broader application for the research question you have answered.>



